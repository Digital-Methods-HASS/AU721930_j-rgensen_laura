---
title: 'Text mining'
date: 'created on 22 November 2020 and updated `r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---
```{r}
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(here)
library(pdftools)
library(lubridate)
```


```{r}

#Indsat ved vejledning fra chatgpt til at kunne forbinde de indsatte filer til at kunne anvende funktionen "analyze()"
library(pdftools)

analyze <- function(file_path) {
  text <- pdf_text(file_path)
  if (length(text) >= 2) {
    cat(text[2])
  } else {
    cat("PDF has fewer than 2 pages.\n") #ønsker kun 2. side i hver dokument
  }
}
```



```{r}
#Følger guiden fra software carpentry 
list.files(path = "data", pattern = "pdf")
```
```{r}
#Følger guiden fra software carpentry 
list.files(path = "data", pattern = "brev_Rosenkrantz", full.names = TRUE)
```
```{r}
#Følger guiden fra software carpentry 
filenames <- list.files(path = "data",  
                        pattern = "brev_Rosenkrantz-[0-9]{2}.pdf",
                        full.names = TRUE)
for (f in filenames) {
  print(f)
  analyze(f)
}
```


```{r}
#Kombinere det med tidytext
analyze <- function(file_path) {
  text <- pdf_text(file_path)
  
  if (length(text) < 2) {
    return(tibble(file = basename(file_path), word = NA))
  }

  second_page <- text[2]
  #ønsker kun 2. side på hver dokument
  
  tibble(file = basename(file_path), text = second_page) %>%
    unnest_tokens(word, text)
}
```


```{r}
filenames <- list.files(path = "data",  
                        pattern = "brev_Rosenkrantz-[0-9]{2}.pdf",
                        full.names = TRUE)

# Use lapply to process all files and bind results
all_words <- lapply(filenames, analyze) %>%
  bind_rows()
```

```{r}
all_words
#indeholder kun for de tre første pdf - måske får mange ord? brug for troubleshooting
```
```{r}
#Tjek op på at alle 9 filer er tilstede 
print(length(filenames)) 
print(basename(filenames))
```

```{r}
#Fjerner stopord 
my_stops <- readLines("stoplist.txt")
my_stops_df <- tibble(word = my_stops)

all_words_clean <- all_words %>%
  filter(!is.na(word)) %>%
  anti_join(my_stops_df, by = "word")

all_words_clean_no_numeric <- all_words_clean %>% 
  filter(is.na(as.numeric(word)))

```

```{r}
view(all_words_clean_no_numeric)
```


Wordcloud for Rosenkrantz

```{r Count words}

all_words_clean_no_numeric %>% 
  count(word) %>% 
  arrange(-n)


```


### A word cloud of the letter words (non-numeric)

See more: https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html

```{r wordcloud-prep}
# There are almost 2000 unique words 
length(unique(all_words_clean_no_numeric$word)) # shows the unique words and count them

# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
all_words_clean_no_numeric_top150 <- all_words_clean_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  slice(1:150) # or head

all_words_clean_no_numeric_top150
```

```{r wordcloud}
all_words_clean_no_numeric_cloud <- ggplot(data = all_words_clean_no_numeric_top150, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

all_words_clean_no_numeric_cloud
```

That's underwhelming. Let's customize it a bit:
```{r wordcloud-pro}
ggplot(data = all_words_clean_no_numeric_top150, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 15) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```


ggplot



```{r}

# Check the unique 2-score words:
unique(all_words_clean_no_numeric_top150$word)

# Count & plot them
all_words_clean_no_numeric_top20 <- all_words_clean_no_numeric %>% 
  count(word, sort = TRUE) %>% #hvorfor sort=true
  mutate(word = fct_reorder(factor(word), -n)) %>%
  slice(1:20)


ggplot(data = all_words_clean_no_numeric_top20, aes(x = word, y = n)) +
  geom_col() +
  coord_flip() +
  theme_bw()
  

# OK so what's the deal with confidence? And is it really "positive" in the emotion sense? 

```

```{r}
ggplot(all_words_clean_no_numeric_top20, aes(x = word, y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Most Used Words",
       x = "Word",
       y = "Frequency") +
  theme_minimal()


```

```{r lollipop chart}
ggplot(all_words_clean_no_numeric_top20, aes(x = word, y = n)) +
  geom_segment(aes(x = word, xend = word, y = 0, yend = n), color = "gray") +
  geom_point(color = "darkred", size = 4) +
  coord_flip() +
  labs(title = "Top 20 Most Used Words (Lollipop Chart)",
       x = "Word", y = "Frequency") +
  theme_minimal()
```
```{r bigram graph}
library(igraph)
library(ggraph)
library(tidytext)

#ændrer det til numeric i stedet for charachter

class(all_words_clean_no_numeric)


all_words_clean_no_numeric_num <- as.numeric(all_words_clean_no_numeric)


# Create bigrams
all_words_clean_no_numeric_bigram <- all_words_clean_no_numeric_top150 %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Count and filter frequent bigrams
bigram_counts_all_words_clean_no_numeric <- all_words_clean_no_numeric_bigram %>%
  count(bigram, sort = TRUE) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  filter(n > 5)

# Create graph object
bigram_graph <- bigram_counts %>%
  graph_from_data_frame()

# Plot network
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
  geom_node_point(color = "skyblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.8, size = 4) +
  theme_void() +
  labs(title = "Word Pair (Bigram) Network")
```

